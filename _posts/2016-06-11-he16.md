---
title: Opponent Modeling in Deep Reinforcement Learning
abstract: Opponent modeling is necessary in multi-agent settings where secondary agents
  with competing goals also adapt their strategies, yet it remains challenging because
  of strategies’ complex interaction and the non-stationary nature. Most previous
  work focuses on developing probabilistic models or parameterized strategies for
  specific applications. Inspired by the recent success of deep reinforcement learning,
  we present neural-based models that jointly learn a policy and the behavior of opponents.
  Instead of explicitly predicting the opponent’s action, we encode observation of
  the opponents into a deep Q-Network (DQN), while retaining explicit modeling under
  multitasking. By using a Mixture-of-Experts architecture, our model automatically
  discovers different strategy patterns of opponents even without extra supervision.
  We evaluate our models on a simulated soccer game and a popular trivia game, showing
  superior performance over DQN and its variants.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: he16
month: 0
tex_title: Opponent Modeling in Deep Reinforcement Learning
firstpage: 1804
lastpage: 1813
page: 1804-1813
order: 1804
cycles: false
bibtex_author: He, He and Boyd-Graber, Jordan and Kwok, Kevin and Daum\'e, III, Hal
author:
- given: He
  family: He
- given: Jordan
  family: Boyd-Graber
- given: Kevin
  family: Kwok
- given: Hal
  family: Daumé
  suffix: III
date: 2016-06-11
address: New York, New York, USA
publisher: PMLR
container-title: Proceedings of The 33rd International Conference on Machine Learning
volume: '48'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 11
pdf: http://proceedings.mlr.press/v48/he16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
